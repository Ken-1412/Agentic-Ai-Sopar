# LLM Service Configuration
# Copy this to .env and configure for your deployment

# ===== LLM Deployment Mode =====
# 'local' = Use Ollama (offline, privacy-focused)
# 'online' = Use HuggingFace (cloud, zero-setup)
LLM_DEPLOYMENT_MODE=online

# ===== Ollama Configuration (Local AI) =====
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_TIMEOUT=60000

# ===== HuggingFace Configuration (Cloud AI) =====
# Model options:
# - meta-llama/Llama-3.2-3B-Instruct (fast, good quality)
# - meta-llama/Llama-2-7b-chat-hf (balanced)
# - mistralai/Mistral-7B-Instruct-v0.2 (alternative)
HF_MODEL=meta-llama/Llama-3.2-3B-Instruct
HF_TIMEOUT=60000
# Optional: HF_API_KEY=hf_... (for higher rate limits)

# ===== Redis Configuration (Caching) =====
REDIS_ENABLED=true
REDIS_URL=redis://localhost:6379

# ===== Generation Parameters =====
MAX_TOKENS=1024
TEMPERATURE=0.7
TOP_P=0.9

# ===== Rate Limiting =====
MAX_REQUESTS_PER_MIN=10
MAX_REQUESTS_PER_HOUR=100
